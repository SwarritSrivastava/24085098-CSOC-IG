{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7WBdSaGa7SH"
      },
      "outputs": [],
      "source": [
        "column_names = [\"rating\", \"title\", \"review\"]\n",
        "import pandas as pd\n",
        "try:\n",
        "  df = pd.read_csv('/content/drive/MyDrive/train.csv', header=None, names=column_names, quotechar='\"')\n",
        "except FileNotFoundError:\n",
        "  print('Error: not found')\n",
        "  exit()\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkO3XzQ7dq91",
        "outputId": "f04d715a-b97e-4bf3-a63e-939e4f66537e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "ratings = df[\"rating\"]\n",
        "titles = df[\"title\"]\n",
        "reviews = df[\"review\"]\n",
        "df[\"rating\"] = df[\"rating\"].map({2: 1, 1: 0})\n",
        "print(df[\"rating\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoM7qqy9iqVU",
        "outputId": "f75bafe9-faea-49a8-bc72-818c0e20d5a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuNtwVmfhSZO"
      },
      "source": [
        "Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y97hBriem926",
        "outputId": "5ac8a9da-fef1-4ebc-d1f6-6cdc89102a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X00ta_LyoESz",
        "outputId": "fc21b008-12ed-4c53-e1c5-2a76845125ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8JnB3Orwkv2",
        "outputId": "9b80e16e-dea8-4d75-b875-c488edbcd188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swifter in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.11/dist-packages (from swifter) (5.9.5)\n",
            "Requirement already satisfied: dask>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2024.12.1)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (4.67.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.7.0)\n",
            "Requirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.1.21)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter) (18.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.22.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install swifter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqlCiyjphYSD"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from spellchecker import SpellChecker\n",
        "import contractions\n",
        "from nltk import FreqDist\n",
        "import swifter\n",
        "swifter.config.compute_parallel = True\n",
        "def tokenizer(raw_String):\n",
        "  raw_String = contractions.fix(raw_String)\n",
        "  # spell = SpellChecker()\n",
        "  # words = raw_String.split()\n",
        "  # misspelled = spell.unknown(words)\n",
        "  # corrected_words = [spell.correction(word) if word in misspelled and spell.correction(word) is not None else word for word in words]\n",
        "  # corrected_text = ' '.join(corrected_words)\n",
        "  # raw_String = corrected_text\n",
        "  # del words\n",
        "  # del spell\n",
        "  # del misspelled\n",
        "  # del corrected_text\n",
        "  # del corrected_words\n",
        "  raw_String = raw_String.replace(\"<br />\", \" \")\n",
        "  raw_String = raw_String.strip()\n",
        "  raw_String = raw_String.lower()\n",
        "  raw_String = re.sub(r'\\d+', '', raw_String)\n",
        "  raw_String = re.sub(r'[^\\w\\s]', '', raw_String)\n",
        "  tokens = word_tokenize(raw_String)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "  freq_words = FreqDist(tokens)\n",
        "  tokens = [word for word in tokens if freq_words[word] < 2]\n",
        "  del freq_words\n",
        "  # stemmer = PorterStemmer()\n",
        "  # tokens = [stemmer.stem(word) for word in tokens]\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "chunk_size = 200_000\n",
        "output_file = \"cleaned_reviews.csv\"\n",
        "column_names = [\"rating\", \"title\", \"review\"]\n",
        "\n",
        "# for i, chunk in enumerate(pd.read_csv('/content/drive/MyDrive/train.csv',chunksize=chunk_size ,header=None, names=column_names, quotechar='\"')):\n",
        "#     print(f\"Processing chunk {i+1}...\")\n",
        "#     chunk[\"clean_review\"] = chunk[\"review\"].swifter.apply(tokenizer)\n",
        "\n",
        "#     if i == 0:\n",
        "#         chunk.to_csv(output_file, index=False)\n",
        "#     else:\n",
        "#         chunk.to_csv(output_file, mode='a', index=False, header=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skpzLgIEW3tG",
        "outputId": "0ad13352-8f90-4356-9bae-9b09b1de9ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sound', 'track', 'beautiful', 'paint']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer(\"This sound track was beautiful! It paints the\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKcoW_PyGO3s",
        "outputId": "d5e0a38a-ee24-4e8c-d995-151d3ad85cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-0159fcdfae6a>:4: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  newdf = pd.read_csv('/content/drive/MyDrive/cleaned_reviews (1).csv', header=None, names=column_names, quotechar='\"')\n",
            "<ipython-input-18-0159fcdfae6a>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf[\"rating\"] = newdf[\"rating\"].astype(int)\n",
            "<ipython-input-18-0159fcdfae6a>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf[\"rating\"] = newdf[\"rating\"].map({2: 1, 1: 0})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   rating                                              title  \\\n",
            "1       2                     Stuning even for the non-gamer   \n",
            "2       2              The best soundtrack ever to anything.   \n",
            "3       2                                           Amazing!   \n",
            "4       2                               Excellent Soundtrack   \n",
            "5       2  Remember, Pull Your Jaw Off The Floor After He...   \n",
            "\n",
            "                                              review  \\\n",
            "1  This sound track was beautiful! It paints the ...   \n",
            "2  I'm reading a lot of reviews saying that this ...   \n",
            "3  This soundtrack is my favorite music of all ti...   \n",
            "4  I truly like this soundtrack and I enjoy video...   \n",
            "5  If you've played the game, you know how divine...   \n",
            "\n",
            "                                      cleaned-review  \n",
            "1  ['sound', 'track', 'beautiful', 'paint', 'sene...  \n",
            "2  ['reading', 'lot', 'review', 'saying', 'best',...  \n",
            "3  ['favorite', 'hand', 'intense', 'sadness', 'pr...  \n",
            "4  ['played', 'relaxing', 'peacefulon', 'disk', '...  \n",
            "5  ['played', 'know', 'divine', 'every', 'single'...  \n",
            "[1 0]\n"
          ]
        }
      ],
      "source": [
        "column_names = [\"rating\", \"title\", \"review\",\"cleaned-review\"]\n",
        "import pandas as pd\n",
        "try:\n",
        "  newdf = pd.read_csv('/content/drive/MyDrive/cleaned_reviews (1).csv', header=None, names=column_names, quotechar='\"')\n",
        "except FileNotFoundError:\n",
        "  print('Error: not found')\n",
        "  exit()\n",
        "newdf = newdf[newdf[\"rating\"] != \"rating\"]\n",
        "newdf[\"rating\"] = newdf[\"rating\"].astype(int)\n",
        "print(newdf.head())\n",
        "newdf[\"rating\"] = newdf[\"rating\"].map({2: 1, 1: 0})\n",
        "print(newdf[\"rating\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newdf[\"cleaned-review\"].apply(len).describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "eybl4kczf1p1",
        "outputId": "8cc4cb5b-7c1b-4ffd-e9e8-0762b7caf309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    3.600000e+06\n",
              "mean     2.905008e+02\n",
              "std      1.607179e+02\n",
              "min      2.000000e+00\n",
              "25%      1.580000e+02\n",
              "50%      2.590000e+02\n",
              "75%      3.960000e+02\n",
              "max      1.121000e+03\n",
              "Name: cleaned-review, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cleaned-review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3.600000e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.905008e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.607179e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.580000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.590000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.960000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.121000e+03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = newdf['rating'].value_counts()\n",
        "\n",
        "print(\"Count of Positive and Negative Classes:\")\n",
        "print(class_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmDUGwBfXg6i",
        "outputId": "58f2837c-7bff-42f9-fd91-d3baab34d2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of Positive and Negative Classes:\n",
            "rating\n",
            "1    1800000\n",
            "0    1800000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-reQyFfIrkT",
        "outputId": "00e77857-9825-403c-ed1b-865d76b44cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of tokens in a review: 300\n"
          ]
        }
      ],
      "source": [
        "max_len = 300\n",
        "print(\"Maximum number of tokens in a review:\", max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4Z-EkLFqC6d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "glove_path = '/content/drive/MyDrive/glove.6B.100d.txt'\n",
        "embedding_dim = 100\n",
        "glove_dict = {}\n",
        "\n",
        "with open(glove_path, 'r', encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.strip().split()\n",
        "        word = values[0]\n",
        "        vector = np.array(values[1:], dtype=np.float32)\n",
        "        glove_dict[word] = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvoVBx96yCxp"
      },
      "outputs": [],
      "source": [
        "hit = 0\n",
        "miss = 0\n",
        "def vectorizer(word):\n",
        "  global hit, miss\n",
        "  if word in glove_dict:\n",
        "    hit += 1\n",
        "    return glove_dict[word]\n",
        "  else:\n",
        "    embedding_dim = 100\n",
        "    miss += 1\n",
        "    return np.zeros(embedding_dim, dtype=np.float32)\n",
        "def list_vectorizer(token_list):\n",
        "  return [vectorizer(word) for word in token_list]\n",
        "column_names = [\"rating\", \"title\", \"review\",\"cleaned-review\"]\n",
        "def pad_vector(vector_list, max_len, embedding_dim):\n",
        "  current_len = len(vector_list)\n",
        "  if current_len == 0:\n",
        "      return np.zeros((max_len, embedding_dim), dtype=np.float32)\n",
        "  elif current_len < max_len:\n",
        "    padding = np.zeros((max_len - current_len, embedding_dim), dtype=np.float32)\n",
        "    padded_sequence = np.concatenate((np.array(vector_list, dtype=np.float32), padding), axis=0)\n",
        "  elif current_len > max_len:\n",
        "    padded_sequence = np.array(vector_list[:max_len], dtype=np.float32)\n",
        "  else:\n",
        "    padded_sequence = np.array(vector_list, dtype=np.float32)\n",
        "  return padded_sequence\n",
        "# for i, chunk in enumerate(pd.read_csv('/content/drive/MyDrive/cleaned_reviews (1).csv',chunksize=chunk_size ,header=None, names=column_names, quotechar='\"',skiprows = 1)):\n",
        "#     print(f\"Processing chunk {i+1}...\")\n",
        "#     chunk.dropna(inplace = True)\n",
        "#     chunk = chunk[chunk[\"rating\"].astype(str) != \"rating\"]\n",
        "#     chunk[\"rating\"] = chunk[\"rating\"].astype(int)\n",
        "#     chunk[\"rating\"] = chunk[\"rating\"].map({2: 1, 1: 0})\n",
        "\n",
        "#     chunk[\"cleaned-review\"] = chunk[\"cleaned-review\"].apply(lambda x: x if isinstance(x, list) else eval(x))\n",
        "#     vectorized_tokens = chunk[\"cleaned-review\"].apply(list_vectorizer)\n",
        "#     padded_sequences = np.array([pad_vector(vec_list, 1121, embedding_dim) for vec_list in vectorized_tokens])\n",
        "#     del vectorized_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "6Wg3WkfpGETW",
        "outputId": "99af753a-099c-4f99-a24f-0ba24bafeb46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m117,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m117,377\u001b[0m (458.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,377</span> (458.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m117,377\u001b[0m (458.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,377</span> (458.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(keras.__version__)\n",
        "import numpy as np\n",
        "max_len = 300\n",
        "embedding_dim = 100\n",
        "model_keras = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(units=128, input_shape=(max_len, embedding_dim)),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_keras.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_keras.summary()\n",
        "import pandas as pd\n",
        "# vectorized_tokens_keras = newdf[\"cleaned-review\"].apply(list_vectorizer)\n",
        "# padded_sequences_keras = np.array([pad_vector(vec_list, max_len, embedding_dim) for vec_list in vectorized_tokens_keras])\n",
        "# del vectorized_tokens_keras\n",
        "# labels_keras = newdf[\"rating\"].values.astype(np.float32)\n",
        "batch_size_keras = 32\n",
        "epochs_keras = 50\n",
        "chunk_size_keras = 500\n",
        "total_reviews_processed = 0\n",
        "for i, chunk_keras in enumerate(pd.read_csv('/content/drive/MyDrive/cleaned_reviews (1).csv',chunksize=chunk_size_keras ,header=None, names=column_names, quotechar='\"',skiprows = 1)):\n",
        "    print(f\"Processing chunk {i+1} for Keras...\")\n",
        "    chunk_keras.dropna(inplace = True)\n",
        "    chunk_keras = chunk_keras[chunk_keras[\"rating\"].astype(str) != \"rating\"]\n",
        "    chunk_keras[\"rating\"] = chunk_keras[\"rating\"].astype(int)\n",
        "    chunk_keras[\"rating\"] = chunk_keras[\"rating\"].map({2: 1, 1: 0})\n",
        "    chunk_keras = chunk_keras.sample(frac=1).reset_index(drop=True)\n",
        "    chunk_keras[\"cleaned-review\"] = chunk_keras[\"cleaned-review\"].apply(lambda x: x if isinstance(x, list) else eval(x))\n",
        "    vectorized_tokens_chunk = chunk_keras[\"cleaned-review\"].apply(list_vectorizer)\n",
        "    padded_sequences_chunk = np.array([pad_vector(vec_list, max_len, embedding_dim) for vec_list in vectorized_tokens_chunk])\n",
        "    labels_chunk = chunk_keras[\"rating\"].values.astype(np.float32)\n",
        "    del vectorized_tokens_chunk\n",
        "    history = model_keras.fit(padded_sequences_chunk, labels_chunk,\n",
        "                    batch_size=64,\n",
        "                    epochs=1,\n",
        "                    verbose=1)\n",
        "    loss = history.history['loss'][0]\n",
        "    accuracy = history.history['accuracy'][0]\n",
        "    total_reviews_processed += len(chunk_keras)\n",
        "    print(f'Chunk [{i+1}] processed. Total so far: {total_reviews_processed} Loss: {loss} Accuracy: {accuracy}')\n",
        "    total_reviews_processed += len(chunk_keras)\n",
        "    if total_reviews_processed >= 250000:\n",
        "        print(f\"Reached target number of reviews ({250000}). Stopping training.\")\n",
        "        break\n",
        "    print(f'Chunk [{i+1}] Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "    print(f\"Hit: {hit} Miss: {miss}\")\n",
        "    hit = 0\n",
        "    miss = 0\n",
        "model_keras.save('my_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "loaded_model = tf.keras.models.load_model('/content/my_model.h5')\n",
        "column_names = [\"rating\", \"title\", \"review\"]\n",
        "import pandas as pd\n",
        "try:\n",
        "  test_df = pd.read_csv('/content/drive/MyDrive/test.csv', header=None, names=column_names, quotechar='\"')\n",
        "except FileNotFoundError:\n",
        "  print('Error: not found')\n",
        "  exit()\n",
        "test_df.dropna(inplace=True)\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "all_true_labels = []\n",
        "all_predicted_labels = []\n",
        "test_file_path = '/content/drive/MyDrive/test.csv'\n",
        "for i, test_chunk in enumerate(pd.read_csv(test_file_path, chunksize=10000, header=None, names=column_names, quotechar='\"')):\n",
        "    print(f\"Processing test chunk {i+1}...\")\n",
        "    test_chunk.dropna(inplace=True)\n",
        "    test_chunk = test_chunk[test_chunk[\"rating\"].astype(str) != \"rating\"]\n",
        "    if test_chunk.empty:\n",
        "        print(f\"Test chunk {i+1} is empty after filtering, skipping.\")\n",
        "        continue\n",
        "    test_chunk[\"rating\"] = test_chunk[\"rating\"].astype(int)\n",
        "    test_chunk[\"rating\"] = test_chunk[\"rating\"].map({2: 1, 1: 0})\n",
        "    test_chunk[\"cleaned-review\"] = test_chunk[\"review\"].apply(tokenizer)\n",
        "    vectorized_tokens_chunk = test_chunk[\"cleaned-review\"].apply(list_vectorizer)\n",
        "    padded_sequences_chunk = np.array([pad_vector(vec_list,  300, 100) for vec_list in vectorized_tokens_chunk])\n",
        "    true_labels_chunk = test_chunk[\"rating\"].values.astype(np.float32)\n",
        "\n",
        "    predictions_chunk = loaded_model.predict(padded_sequences_chunk)\n",
        "\n",
        "    predicted_labels_chunk = (predictions_chunk > 0.5).astype(int)\n",
        "\n",
        "    all_true_labels.extend(true_labels_chunk)\n",
        "    all_predicted_labels.extend(predicted_labels_chunk)\n",
        "\n",
        "print(f\"Processed test chunk {i+1}. Total samples processed so far: {len(all_true_labels)}\")\n",
        "\n",
        "print(\"\\nCalculating overall metrics on the entire test dataset...\")\n",
        "\n",
        "\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "all_predicted_labels = np.array(all_predicted_labels)\n",
        "\n",
        "\n",
        "f1_test_overall = f1_score(all_true_labels, all_predicted_labels)\n",
        "print(f\"\\nOverall F1 Score on Test Data: {f1_test_overall:.4f}\")\n",
        "\n",
        "cm_test_overall = confusion_matrix(all_true_labels, all_predicted_labels)\n",
        "print(\"\\nOverall Confusion Matrix on Test Data:\")\n",
        "print(cm_test_overall)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ifQgQY3rfqe",
        "outputId": "06f1356d-56eb-4ec6-8adf-7ca444eaaf08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test chunk 1...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 126ms/step\n",
            "Processing test chunk 2...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 130ms/step\n",
            "Processing test chunk 3...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 131ms/step\n",
            "Processing test chunk 4...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 127ms/step\n",
            "Processing test chunk 5...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 128ms/step\n",
            "Processing test chunk 6...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 133ms/step\n",
            "Processing test chunk 7...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 87ms/step\n",
            "Processing test chunk 8...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 88ms/step\n",
            "Processing test chunk 9...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 86ms/step\n",
            "Processing test chunk 10...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 88ms/step\n",
            "Processing test chunk 11...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 85ms/step\n",
            "Processing test chunk 12...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 86ms/step\n",
            "Processing test chunk 13...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 90ms/step\n",
            "Processing test chunk 14...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 88ms/step\n",
            "Processing test chunk 15...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step\n",
            "Processing test chunk 16...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 95ms/step\n",
            "Processing test chunk 17...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 93ms/step\n",
            "Processing test chunk 18...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 90ms/step\n",
            "Processing test chunk 19...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 88ms/step\n",
            "Processing test chunk 20...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 88ms/step\n",
            "Processing test chunk 21...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 90ms/step\n",
            "Processing test chunk 22...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step\n",
            "Processing test chunk 23...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 88ms/step\n",
            "Processing test chunk 24...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 88ms/step\n",
            "Processing test chunk 25...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 90ms/step\n",
            "Processing test chunk 26...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 89ms/step\n",
            "Processing test chunk 27...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 88ms/step\n",
            "Processing test chunk 28...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 88ms/step\n",
            "Processing test chunk 29...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 100ms/step\n",
            "Processing test chunk 30...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 155ms/step\n",
            "Processing test chunk 31...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 149ms/step\n",
            "Processing test chunk 32...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 150ms/step\n",
            "Processing test chunk 33...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 150ms/step\n",
            "Processing test chunk 34...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 144ms/step\n",
            "Processing test chunk 35...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 145ms/step\n",
            "Processing test chunk 36...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 131ms/step\n",
            "Processing test chunk 37...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 119ms/step\n",
            "Processing test chunk 38...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 100ms/step\n",
            "Processing test chunk 39...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 89ms/step\n",
            "Processing test chunk 40...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 116ms/step\n",
            "Processed test chunk 40. Total samples processed so far: 399976\n",
            "\n",
            "Calculating overall metrics on the entire test dataset...\n",
            "\n",
            "Overall F1 Score on Test Data: 0.0000\n",
            "\n",
            "Overall Confusion Matrix on Test Data:\n",
            "[[199984      0]\n",
            " [199992      0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_test_overall, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Overall Confusion Matrix on Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nt0QE7vI7xeW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}